{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49033067-874d-4215-9e8a-2fdde55f27df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20NG Text Dataset:\n",
      "Training Accuracy: 0.5336084905660378\n",
      "Testing Accuracy: 0.2676392572944297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST Dataset:\n",
      "Training Accuracy: 0.9803174603174604\n",
      "Testing Accuracy: 0.9689285714285715\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups, fetch_openml\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Load the 20NG text dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all')\n",
    "\n",
    "# Convert the text data into TF-IDF features\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(newsgroups.data)\n",
    "y = newsgroups.target\n",
    "\n",
    "\n",
    "# Split the data into train and test sets with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Further split the train set into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train, random_state=42)\n",
    "\n",
    "\n",
    "# Normalize the TF-IDF feature values (zero mean, unit variance)\n",
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_val_normalized = scaler.transform(X_val)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# Train KNN classifier on the 20NG text dataset\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_normalized, y_train)\n",
    "\n",
    "# Predict on the training set\n",
    "y_train_pred = knn.predict(X_train_normalized)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = knn.predict(X_test_normalized)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"20NG Text Dataset:\")\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Testing Accuracy:\", test_accuracy)\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True, as_frame=False)\n",
    "X = mnist.data.astype('float32')\n",
    "y = mnist.target.astype('int')\n",
    "\n",
    "\n",
    "# Split the data into train and test sets with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Further split the train set into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train, random_state=42)\n",
    "\n",
    "\n",
    "# Normalize the pixel values to have a mean of 0\n",
    "scaler = StandardScaler(with_mean=True, with_std=False)\n",
    "X_train_normalized = scaler.fit_transform(X_train)\n",
    "X_val_normalized = scaler.transform(X_val)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "\n",
    "# Train KNN classifier on the MNIST dataset\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train_normalized, y_train)\n",
    "\n",
    "# Predict on the training set\n",
    "y_train_pred = knn.predict(X_train_normalized)\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = knn.predict(X_test_normalized)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"MNIST Dataset:\")\n",
    "print(\"Training Accuracy:\", train_accuracy)\n",
    "print(\"Testing Accuracy:\", test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ebee59-4d73-4cb9-99bc-1ce30c64d1b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
